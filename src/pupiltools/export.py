# Copyright 2025 Shane Forbrigger
# Licensed under the MIT License (see LICENSE file in project root)

"""Functions to export data from Pupil Core recordings"""

import msgpack as mpk
import numpy as np
from pathlib import Path
from collections.abc import Iterator, Container
import csv
import json
from typing import Any
from .data_structures import PupilData, GazeData, flatten_nested_tuple, PUPIL_CSV_FIELDS, GAZE_CSV_FIELDS
from .aliases import pupil_datatype, gaze_datatype, ResampledTrialDataType
from .utilities import fix_datetime_string, make_digit_str
import h5py
from os import PathLike
from .constants import DATA_FILE_SUFFIX, TS_FILE_SUFFIX


class FolderNotFoundError(Exception):
    pass


def export_folder(folder_path: Path, output_path: Path, filetype: str = "csv", experiment_log: Path | None = None, demographics_log: Path | None = None):
    """Mass export folders of Pupil Capture recordings

    Given a folder containing Pupil Capture recording folders, export all recordings
    to a more convenient file format in a single location.

    Parameters
    ----------
    folder_path: pathlib.Path
        The folder containing all recording folders to be exported. The recording 
        folders should have been generated by Pupil Capture and contain the 
        corresponding``.pldata`` and ``.npy`` files.
    output_path: pathlib.Path
        The folder to store the exported file(s).
    filetype: {"csv", "hdf"}
        Type of the output file(s). If ``"csv"``, then the output will be one file
        per topic (pupil and gaze) per recording folder exported. If ``"hdf"``, then a
        single HDF5 file will be generated with an internal file structure separating
        each recording.
    experiment_log: pathlib.Path, optional
        A path to a JSON log file that describes which folders on the folder_path
        should be exported and describes the metadata of those recordings.

        REQUIRED for HDF file export since it contains the trial metadata. Optional for 
        CSV export. See :py:func:`pupiltools.export.get_metadata` for an example of the
        contents of an experiment log file.
    demographics_log: pathlib.Path, optional
        A path to a JSON log file that describes the participant demographics
        associated with the set of recordings given by ``experiment_log``. It uses
        the ``participant_id`` field in the header of the experiment log to determine
        which demographic data to extract. This parameter is only used for HDF file
        export.
        
        See :py:func:`pupiltools.export.get_metadata` for an example of the contents of 
        a demographics log file and corresponding experiment log file.
    """
    assert folder_path.exists()
    if experiment_log is not None:
        sub_folders = get_subfolders_from_log(folder_path, experiment_log)
    else:
        # Find all subfolders within the folder_path
        sub_folders = (subdir for subdir in folder_path.iterdir() if subdir.is_dir())
    if not output_path.exists():
        output_path.mkdir(parents=True)
    topics = ("pupil", "gaze")
    if filetype == "csv":
        for sub_folder in sub_folders:
            export_data_csv(sub_folder, output_path, topics)
    elif filetype == "hdf":
        assert isinstance(experiment_log, Path) and isinstance(demographics_log, Path)
        metadata = get_metadata(experiment_log, demographics_log)
        # Fix typo in the experiment log's datetime string
        metadata["header"]["date"] = fix_datetime_string(metadata["header"]["date"])
        export_hdf_from_raw(folder_path, output_path, sub_folders, metadata, topics)


def get_subfolders_from_log(folder_path: Path, experiment_log: Path) -> Iterator[Path]:
    with open(experiment_log) as f:
        log_data = json.load(f)
    for item in log_data["trial_record"]:
        subfolder_path = folder_path / Path(item["recording"])
        if subfolder_path.exists():
            yield subfolder_path
        else:
            raise FolderNotFoundError(f"Recording folder {subfolder_path} not found.")


def export_data_csv(
    folder_path: Path, output_path: Path, data_topics: tuple[str, ...]
):
    """Export raw Pupil and/or Gaze pldata and npy files to CSV

    Depending on the topics supplied to the function, it will output one or two files
    containing the pupil and/or gaze data.
    
    Parameters
    ----------
    folder_path: pathlib.Path
        Path to a single Pupil Capture recording folder containing pldata and npy files.
    output_path: pathlib.Path
        Path to a folder to place the output CSV file(s). The CSV files will be named
        based on their topic and the last folder in ``folder_path``. For example, if
        ``folder_path = C:/Users/user/recordings/P01/004/`` and ``topic = ("pupil", 
        "gaze")``, then the output files will be ``004_pupil_positions.csv`` and
        ``004_gaze_positions.csv``.
    data_topics: tuple[str, ...]
        A tuple, or any iterable, of strings describing the topics to be exported as
        CSV files. Currently the only options are "pupil", "gaze", or both.
    """
    world_ts_data = np.load(folder_path / ("world" + TS_FILE_SUFFIX))
    for topic in data_topics:
        data_file = folder_path / f"{topic}{DATA_FILE_SUFFIX}"
        export_file = output_path / f"{folder_path.name}_{topic}_positions.csv"
        if topic == "pupil":
            csv_fieldnames = PUPIL_CSV_FIELDS
        else:
            csv_fieldnames = GAZE_CSV_FIELDS
        with open(export_file, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=csv_fieldnames)
            # Data processing pipeline using generators
            data = extract_data(data_file, t_start=world_ts_data[0], topic=topic, method="3d")
            timestamped_data = match_timestamps(data, world_ts_data)
            flattened_data = flatten_data(timestamped_data)
            writer.writeheader()
            writer.writerows(flattened_data)


def export_hdf_from_raw(folder_path: Path, output_path: Path, sub_folders: Iterator[Path], metadata: dict, topics: tuple[str, ...] = ("pupil",)):
    """Export raw Pupil pldata and npy files to HDF5 format"""
    export_file = output_path / f"{folder_path.name}.hdf5"
    with h5py.File(export_file, 'w') as f_root:
        f_root.attrs.update(metadata["header"])
        trials_group = f_root.create_group("trials")
        for trial_num, sub_folder in enumerate(sub_folders):
            trial_group = trials_group.create_group(make_digit_str(trial_num))
            trial_group.attrs.update(metadata["trial_record"][trial_num])
            world_ts_data = np.load(sub_folder / f"world{TS_FILE_SUFFIX}")
            for topic in topics:
                data_file = sub_folder / f"{topic}{DATA_FILE_SUFFIX}"
                data = extract_data(data_file, world_ts_data[0], topic=topic, method="3d")
                timestamped_data = match_timestamps(data, world_ts_data)
                grouped_data = {}
                for data_entry in timestamped_data:
                    method = data_entry.topic.split(".")[-1]
                    dataset_name = topic
                    if isinstance(data_entry, PupilData):
                        dataset_name = dataset_name + f"_eye{data_entry.id}_{method}"
                    if dataset_name not in grouped_data:
                        grouped_data[dataset_name] = []
                    grouped_data[dataset_name].append(data_entry.fields_to_tuple())
                for dataset_name, dataset_values in grouped_data.items():
                    name_parts = dataset_name.split("_")
                    dataset_metadata = {"topic": name_parts[0]}
                    if name_parts[0] == "pupil":
                        dataset_metadata.update({
                            "eye id": name_parts[1][-1],
                            "method": "pye3d 0.3.0 real-time",
                        })
                        dtype = pupil_datatype
                    else:
                        assert name_parts[0] == "gaze"
                        dtype = gaze_datatype
                    dataset_values = np.array(dataset_values, dtype=dtype)
                    dataset = trial_group.create_dataset(dataset_name, data=dataset_values)
                    dataset.attrs.update(dataset_metadata)


def extract_data(
    data_file: Path, t_start: float, topic: str, method: str = "3d"
) -> Iterator[PupilData | GazeData]:
    """Extract Pupil data from pldata (msgpack) format"""
    with open(data_file, "rb") as f:
        unpacker = mpk.Unpacker(f, use_list=False)
        msg_topic: str
        for msg_topic, b_obj in unpacker:
            main_topic = msg_topic.split(".")[0]
            if topic == "pupil" and main_topic == topic:
                # Topic strings are of the form pupil.[eye_id].[method_id]
                if msg_topic.split(".")[2] == method:
                    data = mpk.unpackb(b_obj)
                    data = PupilData(**data)
                    if data.timestamp >= t_start:
                        yield data
            elif topic == "gaze" and main_topic == topic:
                # Topic strings are of the form gaze.3d.01
                data = mpk.unpackb(b_obj)
                data = GazeData(**data)
                if data.timestamp >= t_start:
                    yield data


def match_timestamps(data: Iterator[PupilData | GazeData], ts_data: np.ndarray) -> Iterator[PupilData | GazeData]:
    """Match data points to the nearest world frame index

    Adapted from:
    https://stackoverflow.com/a/8929827
    """
    for entry in data:
        target = entry.timestamp
        # Find the index where ts_data[idx-1] < target <= ts_data[idx]
        idx = ts_data.searchsorted(target)
        # If the closest index is 0 or the length of the list, clip to 1 or length - 1
        idx = np.clip(idx, 1, len(ts_data) - 1)
        left = ts_data[idx - 1]
        right = ts_data[idx]
        # Change idx to match the closer of the left or right index
        idx -= target - left < right - target
        entry.world_index =  int(idx)
        yield entry


def flatten_data(data: Iterator[PupilData | GazeData]) -> Iterator[dict[str, Any]]:
    """Turn an iterator of nested dicts into an iterator single-layer dicts"""
    for entry in data:
        entry_data = entry.fields_to_tuple_for_csv()
        entry_data = flatten_nested_tuple(entry_data)
        match entry:
            case PupilData():
                labels = PUPIL_CSV_FIELDS
            case GazeData():
                labels = GAZE_CSV_FIELDS
        labelled_data = dict(zip(labels, entry_data))
        yield labelled_data


def get_metadata(experiment_log: Path, demographics_log: Path) -> dict:
    """Get experiment and participant metadata from log files
    
    Parameters
    ----------
    experiment_log : pathlib.Path
        Path to the experiment log file, a .json file with a header dictionary 
        containing the participant ID and datetime of the experiment, then a list of 
        trial record dictionaries giving the trial number, the task type, which die was
        used, the associated recording number, the start time, the instruction time, and
        the stop time of the experiment. For example::

            {
                "header": {
                    "participant_id": "P01",
                    "date": "2024-08-15-T09:58:07"
                },
                "trial_record": [
                    {
                        "trial": 0,
                        "task": "observation",
                        "die": "4",
                        "recording": "010",
                        "t_start": 204.30433799998718,
                        "t_instruction": 207.1633179999917,
                        "t_stop": 208.1201849999925
                    }, 
                    {
                        "trial": 1,
                        "task": "observation",
                        "die": "2",
                        "recording": "011",
                        "t_start": 217.57487699999183,
                        "t_instruction": 220.36736999999266,
                        "t_stop": 221.31473499999265
                    }, ...
                ]
            }

    demographics_log : pathlib.Path
        Path to the demographics log for all experiments, a .json file with entries
        corresponding to each participant ID. Each entry gives the age group, dominant
        hand, and gender of the participant. For example::

            {
                "P01":{
                    "Age Group": "18 to 24",
                    "Dominant Hand": "Right",
                    "Gender": "Man"
                },
                "P02":{
                    "Age Group": "45 to 64",
                    "Dominant Hand": "Left",
                    "Gender": "Woman"
                }, ...
            }
    
    """
    experiment_metadata: dict[str, dict] = load_json_log(experiment_log)
    participant_id = experiment_metadata["header"]["participant_id"]
    demographic_metadata = load_json_log(demographics_log)
    demographic_metadata = demographic_metadata[participant_id]
    experiment_metadata["header"].update(demographic_metadata)
    return experiment_metadata


def export_hdf(file:str | bytes | PathLike, data_structure: dict[str, dict | list]):
    """Export a dictionary of data and attributes to an HDF File
    
    Parameters
    ----------
    file: str | bytes | os.PathLike
        A file to export to. Either a path as a string, or a file-like object from 
        open() or similar functions.
    data_structure: dict[str, dict | list]
        A dictionary containing data and attributes of the data. It is expected to have
        the following structure::

            {
                "attributes":
                    {
                        "attr1": val1,
                        etc
                    },
                "data":
                    [
                        {
                            "attributes":
                            {
                                "attr1": val1,
                                etc
                            },
                            "data": np.ndarray
                        },
                        {
                            "attributes":
                            {
                                "attr1": val1,
                                etc
                            },
                            "data": np.ndarray
                        }, ...
                    ]
            }
    """
    with h5py.File(file, 'w') as f_root:
        f_root.attrs.update(data_structure["attributes"])
        trials_group = f_root.create_group("trials")
        for i, trial in enumerate(data_structure["data"]):
            trial_group = trials_group.create_group(make_digit_str(i))
            trial_group.attrs.update(trial["attributes"])
            trial_group.create_dataset("pupil_3d", data=trial["data"])


def recurse_write_hdf(group: h5py.Group, data_structure):
    attrs_key = "attributes"
    if isinstance(data_structure, dict):
        for key, item in data_structure.items():
            if key == attrs_key:
                group.attrs.update(item)
            elif isinstance(item, np.ndarray):
                group.create_dataset(key, data=item)
            elif isinstance(item, Iterator):
                subgroup = group.create_group(key)
                recurse_write_hdf(subgroup, item)


def load_json_log(log_file_path: Path) -> dict:
    with open(log_file_path, "r") as f:
        log_data = json.load(f)
    return log_data


def save_processed_data(output_path: Path, processed_data: dict[str, dict], attributes: dict[str, Any]):
    with h5py.File(output_path, 'w') as f_root:
        f_root.attrs.update(attributes)
        for key, tasks in processed_data.items():
            p_group = f_root.create_group(key)
            for task, task_data in tasks.items():
                p_group.create_dataset(task, data=task_data)
