# Eye Tracking and Pupillometry for Motor Intent Detection

**Lead researcher**: Dr. Shane Forbrigger, Department of Mechanical Engineering, 
Dalhousie University

**Other researchers**:

Prof. Ya-Jun Pan (supervisor), Department of Mechanical Engineering, Dalhousie 
University

Prof. Thomas Trappenberg, Department of Computer Science, Dalhousie University

**Funding**: We acknowledge the support of the Natural Sciences and Engineering Research 
Council of Canada (NSERC). Nous remercions le Conseil de recherches en sciences 
naturelles et en génie du Canada (CRSNG) de son soutien.

**Associated Paper**: S. Forbrigger, T. Trappenberg and Y. -J. Pan, "Pupillometry for 
Arm and Hand Motor Intent Detection," *2025 International Conference On Rehabilitation 
Robotics (ICORR)*, Chicago, IL, USA, 2025, pp. 1382-1387, doi: 
[10.1109/ICORR66766.2025.11063027](https://doi.org/10.1109/ICORR66766.2025.11063027).

**Associated Dataset**: 
Forbrigger, Shane, 2025, "Eye Tracking and Pupillometry for Motor Intent Detection",
https://doi.org/10.5683/SP3/SET9FR, Borealis.

## Introduction
The purpose of this study is to learn about the differences between eye movements and 
changes in pupil size when picking up objects versus observing objects. We collected
gaze tracking and pupillometric data to train an algorithm to distinguish a person’s 
intended action (pick up or observe an object) based solely on their eye behaviour. This 
algorithm will serve as a proof-of-concept for an intuitive controller of assisted 
devices for people with arm motor impairments.

Participants in the study were asked to perform a series of simple tasks while their 
eyes and the scene in front of them, including their hands, were video-recorded. The 
recordings were captured using a Pupil Core headset (https://pupil-labs.com/products/core).
The tasks involved picking up dice from a table and reading numbers off those dice.

See [Study Details](docs/study-details.md) for more information on the participant 
criteria and methodology.

## How to use
Navigate your terminal to the project root directory. I recommend creating a virtual
environment with `venv` or `conda` to keep the installed packages separate from your
main Python distrobution.

If you only want to work on the pupiltools package, install using `pip` in editable
mode:

```
python -m pip install -e .
```

If you want to run the scripts, you can install using the included `requirements.txt`
file:

```
python -m pip install -r requirements.txt
```

The package included with this project is named `pupiltools`. See the scripts in the
`/scripts` folder for examples of use.

### Building documentation
The documentation for the `pupiltools` package in this project is written to be 
compatible with [sphinx](https://www.sphinx-doc.org/en/master/) autodocumentation. To 
install the documentation requirements, either install using the `requirements.txt` 
file, or install the pupiltools package with the `docs` optional dependencies:

```
pip install -e .[docs]
```

You can build the documentation from the command line. With your virtual environment
active, enter the following command while in the project root directory:

```
sphinx-build ./docs/source/ ./docs/build
```

This publishes the html files in the `./docs/build` folder. You can change the build
destination folder to a different location if you prefer. If you open this folder
you will find an `index.html` file. Open this file in your browser to access the 
documentation.

If you make changes to the documentation you may need to do a full rebuild to make sure
that all tables of contents and other autogenerated content updates properly. To do so,
run

```
sphinx-build -aE ./docs/source/ ./docs/build
```

### Using the scripts
Each script in the ``/scripts`` folder performs a single task in a data processing/
analysis pipeline. The order of operation of the scripts is:

0. ``split_session_log_fix_script.py`` - One of the experiment sessions was interrupted
so the data and metadata was split between multiple folders. This script was used to 
merge the data into a single folder with consistent metadata, making the data appear 
like it was never split.
1. ``export_script.py`` - take raw participant recordings made by Pupil Capture and
export them to HDF. One HDF file per participant. The resulting HDF files are the data 
made available in the Borealis repository.
    * ``time_difference_script.py`` - produce a histogram of time intervals between
    samples for a single participant.
    * ``max_diameter_script.py`` - produce a histogram of maximum pupil diameter
    separated by task type for all participants, with the diameter value of maximum 
    class separation labelled.
2. ``resample_script.py`` - take HDF files from the previous step and resample the data
to a constant sample rate (original data has a variable sample rate). Produces one
HDF file per participant.
    * ``instruction_time_scripy.py`` - produce a histogram of timestamps for when
    instructions were delivered to each participant in every trial, separated by task
    type.
    * ``pupil_stats_script.py`` - produce pupil diameter trendlines and max diameter
    histogram for a single participant.
3. ``processing_script.py`` - take the HDF files with resampled data, remove data below
the minimum confidence threshold, linearly interpolate the removed data, and normalize
the pupil diameter measurements. Produces one HDF file containing all participant
data.
    * ``saccades_script.py`` - Deprecated, originally the ``processing_script.py``
    applied a low-pass filter to the data but this was found to be unnecessary. This 
    script was used to create plots that compared the filtered vs unfiltered data to 
    determine if filtering was necessary.
4. ``extract_features_script.py`` or ``extract_timeseries_script.py`` - Extract data
from the processed participant data into a numpy arrays conducive to neural network 
input (including labels). The features script calculates four statistics per eye per 
task to serve as neural net input (this is what was used in the conference paper) while
the timeseries script extracts the full timeseries for selected variables.
    * ``conference_paper/plot_features.py`` - produce the feature histogram plots
    (Fig. 3 in the conference paper).
    * ``plotting.py`` - produce the pupil diameter trendline plots (Fig. 2 in the
    conference paper).
5. ``classification_script.py`` - Train and test a neural network on data from step 4 
and save the results.
6. ``results_script.py`` - Produce confusion matrices and score histograms (Fig. 4 and
5 in the paper) and calculate the accuracy and F1-score statistics.

All scripts require command line arguments. Help information for each script can be 
displayed using the following command with the virtual environment active:

```
python [script_name] -h
```

For example:
```
> python .\scripts\processing_script.py -h
usage: processing_script.py [-h] [--confidence_threshold CONFIDENCE_THRESHOLD] data_path export_path filter_config_file

positional arguments:
  data_path             Path to directory containing the HDF files.
  export_path           Path to directory to save the processed data.
  filter_config_file    Path to YAML file with filter configurations for each variable to be filtered.

options:
  -h, --help            show this help message and exit
  --confidence_threshold CONFIDENCE_THRESHOLD
                        Eye tracker confidence value below which data is discarded.
```
